{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:42:15.694595Z","iopub.execute_input":"2025-03-12T02:42:15.694791Z","iopub.status.idle":"2025-03-12T02:42:21.860880Z","shell.execute_reply.started":"2025-03-12T02:42:15.694773Z","shell.execute_reply":"2025-03-12T02:42:21.860094Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 1 定义数据集","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self,split):\n        self.dataset=load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self,i):\n        text=self.dataset[i]['text']\n        label=self.dataset[i]['label']\n\n        return text,label\n\ndataset=Dataset('train')\nlen(dataset),dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:42:21.861652Z","iopub.execute_input":"2025-03-12T02:42:21.862089Z","iopub.status.idle":"2025-03-12T02:42:25.465392Z","shell.execute_reply.started":"2025-03-12T02:42:21.862060Z","shell.execute_reply":"2025-03-12T02:42:25.464508Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/960 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"855ec7d397794e1f96bd14f2a80ff9ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-02f200ca5f2a7868.parquet:   0%|          | 0.00/2.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c01252c8dd4e47b68560d8af75499c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-405befbaa3bcf1a2.parquet:   0%|          | 0.00/276k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ee0d103ce64d8c859b4700274c13ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-5372924f059fe767.parquet:   0%|          | 0.00/275k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a9ac6487fc4aafbe3cb1b86464b92a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d9fa3c643794a21aad6bdbfa043bdfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2fe479e07d64b7db1e16b69966b88f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd2501076cd6406fa1c917500a025eb9"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"(9600,\n ('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n  1))"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"### 2 加载字典和分词工具","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntoken = BertTokenizer.from_pretrained('bert-base-chinese')\n\ntoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:42:25.466060Z","iopub.execute_input":"2025-03-12T02:42:25.466400Z","iopub.status.idle":"2025-03-12T02:42:30.937844Z","shell.execute_reply.started":"2025-03-12T02:42:25.466380Z","shell.execute_reply":"2025-03-12T02:42:30.937163Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd930914785e400e94d3247c692f0cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e828a60240e45d4a12d9459655bdee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c7112e26d7f4a3dbfc11c9a81fe0d7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e93577f0834582b3e0d145a6592614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f888ee0338fa4d97a1e8ac9d2916af9a"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### 3 数据加载器","metadata":{}},{"cell_type":"code","source":"def collate_fn(data):\n    sents=[i[0] for i in data]\n    labels=[i[1] for i in data]\n\n    #编码\n    data=token.batch_encode_plus(batch_text_or_text_pairs=sents,\n                                truncation=True,\n                                padding='max_length',\n                                max_length=500,\n                                return_tensors='pt',\n                                return_length=True,\n                                )\n\n    #input_ids:编码之后的数字\n    #attention_mask:是补零的位置是0,其他位置是1\n    input_ids=data['input_ids']\n    attention_mask = data['attention_mask']\n    token_type_ids = data['token_type_ids']\n\n    labels=torch.LongTensor(labels)\n\n    return input_ids, attention_mask, token_type_ids, labels\n\n#数据加载器\nloader=torch.utils.data.DataLoader(dataset=dataset,\n                                  batch_size=16,\n                                  collate_fn=collate_fn,\n                                  shuffle=True,\n                                  drop_last=True)\n\nfor i,(input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n    break\n\nprint(len(loader))\ninput_ids.shape, attention_mask.shape, token_type_ids.shape, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:55:32.347091Z","iopub.execute_input":"2025-03-12T02:55:32.347390Z","iopub.status.idle":"2025-03-12T02:55:32.504685Z","shell.execute_reply.started":"2025-03-12T02:55:32.347365Z","shell.execute_reply":"2025-03-12T02:55:32.503855Z"}},"outputs":[{"name":"stdout","text":"600\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(torch.Size([16, 500]),\n torch.Size([16, 500]),\n torch.Size([16, 500]),\n tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0]))"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### 4 模型","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n\n#加载预训练模型\npretrained=BertModel.from_pretrained('bert-base-chinese')\n#不训练,不需要计算梯度\nfor param in pretrained.parameters():\n    param.requires_grad_(False)\n\n#模型试算\nout = pretrained(input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 token_type_ids=token_type_ids)\nout.last_hidden_state.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:08:14.983279Z","iopub.execute_input":"2025-03-12T03:08:14.983608Z","iopub.status.idle":"2025-03-12T03:08:43.084455Z","shell.execute_reply.started":"2025-03-12T03:08:14.983582Z","shell.execute_reply":"2025-03-12T03:08:43.083646Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94db6e9be0894bb492d6caec05c67e55"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 500, 768])"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### 5 定义下游任务模型","metadata":{}},{"cell_type":"code","source":"class Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc=torch.nn.Linear(768,2)\n\n    def forward(self,input_ids,attention_mask,token_type_ids):\n        with torch.no_grad():\n            out = pretrained(input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 token_type_ids=token_type_ids)\n\n        out=self.fc(out.last_hidden_state[:,0])\n        out=out.softmax(dim=1)\n\n        return out\n\nmodel=Model()\n\nmodel(input_ids=input_ids,\n      attention_mask=attention_mask,\n      token_type_ids=token_type_ids).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:11:50.018938Z","iopub.execute_input":"2025-03-12T03:11:50.019249Z","iopub.status.idle":"2025-03-12T03:12:00.369883Z","shell.execute_reply.started":"2025-03-12T03:11:50.019228Z","shell.execute_reply":"2025-03-12T03:12:00.369025Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 2])"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### 6 训练","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW\n\noptimizer=AdamW(model.parameters(),lr=5e-4)\ncriterion=torch.nn.CrossEntropyLoss()\n\nmodel.train()\n\nfor i, (input_ids, attention_mask, token_type_ids,\n        labels) in enumerate(loader):\n    out = model(input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids)\n    \n    loss=criterion(out,labels)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    if i%5==0:\n        out=out.argmax(dim=1)\n        accuracy=(out==labels).sum().item()/len(labels)\n\n        print(i,loss.item(),accuracy)\n    \n    if i == 300:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:17:06.085905Z","iopub.execute_input":"2025-03-12T03:17:06.086194Z","iopub.status.idle":"2025-03-12T04:08:56.965022Z","shell.execute_reply.started":"2025-03-12T03:17:06.086172Z","shell.execute_reply":"2025-03-12T04:08:56.964119Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"0 0.7590072751045227 0.375\n5 0.6746047735214233 0.5625\n10 0.6309472918510437 0.6875\n15 0.6731126308441162 0.6875\n20 0.6028311848640442 0.75\n25 0.6080811619758606 0.625\n30 0.6647576689720154 0.5\n35 0.520226001739502 1.0\n40 0.5909832119941711 0.6875\n45 0.5008869171142578 0.9375\n50 0.4827878475189209 0.9375\n55 0.5563780069351196 0.875\n60 0.48446062207221985 0.875\n65 0.47542521357536316 0.875\n70 0.4827728569507599 0.9375\n75 0.49847108125686646 0.8125\n80 0.4473411440849304 0.875\n85 0.4774356186389923 0.875\n90 0.4438154101371765 0.9375\n95 0.43533143401145935 0.9375\n100 0.48415639996528625 0.875\n105 0.5001518726348877 0.75\n110 0.5333747863769531 0.75\n115 0.47413820028305054 0.875\n120 0.4978610575199127 0.875\n125 0.5136285424232483 0.875\n130 0.5017361640930176 0.8125\n135 0.48214778304100037 0.8125\n140 0.4277268052101135 0.9375\n145 0.5093130469322205 0.875\n150 0.4964030683040619 0.8125\n155 0.6243466138839722 0.6875\n160 0.48617786169052124 0.75\n165 0.4418548345565796 0.9375\n170 0.40361934900283813 0.9375\n175 0.466561883687973 0.875\n180 0.44522878527641296 0.875\n185 0.4148256182670593 0.9375\n190 0.5826003551483154 0.6875\n195 0.4371708631515503 0.875\n200 0.614971399307251 0.6875\n205 0.48233121633529663 0.875\n210 0.44358575344085693 0.875\n215 0.4014107584953308 0.9375\n220 0.38592472672462463 0.9375\n225 0.4202612638473511 0.875\n230 0.4111069440841675 0.9375\n235 0.5806137919425964 0.6875\n240 0.4398702383041382 0.875\n245 0.3996492624282837 1.0\n250 0.37827327847480774 1.0\n255 0.40103352069854736 1.0\n260 0.40860074758529663 0.875\n265 0.5136756300926208 0.8125\n270 0.39948800206184387 0.9375\n275 0.5737124681472778 0.6875\n280 0.6029447913169861 0.6875\n285 0.46900561451911926 0.875\n290 0.42558416724205017 0.9375\n295 0.4474238455295563 0.875\n300 0.5854877829551697 0.6875\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 7 测试","metadata":{}},{"cell_type":"code","source":"def test():\n    model.eval()\n    correct=0\n    total=0\n    \n    loader_test=torch.utils.data.DataLoader(dataset=Dataset('validation'),\n                                  batch_size=32,\n                                  collate_fn=collate_fn,\n                                  shuffle=True,\n                                  drop_last=True)\n\n    for i, (input_ids, attention_mask, token_type_ids,\n        labels) in enumerate(loader_test):\n        if i==5:\n            break\n        print(i)\n\n        with torch.no_grad():\n            out = model(input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 token_type_ids=token_type_ids)\n        out=out.argmax(dim=1)\n        correct+=(out==labels).sum().item()\n        total+=len(labels)\n\n    print(correct/total)\n        \ntest()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T04:10:49.896506Z","iopub.execute_input":"2025-03-12T04:10:49.896838Z","iopub.status.idle":"2025-03-12T04:12:42.132162Z","shell.execute_reply.started":"2025-03-12T04:10:49.896811Z","shell.execute_reply":"2025-03-12T04:12:42.131371Z"}},"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n0.85625\n","output_type":"stream"}],"execution_count":13}]}